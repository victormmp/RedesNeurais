\documentclass{article}

\usepackage[utf8x]{inputenc}
\usepackage{ucs}
\usepackage[portuguese]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{geometry}
\usepackage{float}
\usepackage{makecell}
\usepackage{hyperref}
\usepackage{multirow}
%\usepackage{subfigure}
\usepackage[table,xcdraw]{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cases}
%\usepackage[framed,numbered,autolinebreaks,useliterate]{matlab-prettifier}
%\usepackage{mcode}
%\usepackage{filecontents}
\graphicspath{{Imagens/}}

% Mudando a fonte do documento para Times New Roman (ptm)
\renewcommand*\rmdefault{ptm}

\pagestyle{fancy}
\fancyhf{}
\lhead[]{Redes Neurais Artificiais}
\rhead[]{Relatório II}
\chead[]{}

\lfoot{Página \thepage \enspace de \pageref{LastPage} }
\rfoot{\leftmark}
\renewcommand{\footrulewidth}{1pt}

\begin{document}
\SweaveOpts{concordance=TRUE}

\begin{center}

{\scshape\Large Universidade Federal de Minas Gerais \par}
{\scshape\large Redes Neurais Artificiais \par}
\vspace{5cm}

\hrule
\hfill

{\huge \textbf{ELM e RBF}\par}
\hfill
\hrule
\hfill

\vspace{3cm}

{\large\itshape Victor Marcius Magalhães Pinto\\Mat: 2019717730\par}

\vspace{2cm}

\end{center}

\newpage

\section{Introdução}

Os exercícios propostos tem por objetivo exercitar um maior conhecimento a respeito de implementações de redes neurais multicamadas, particularmente de técnicas de cálculo dos pesos. Redes ELM \textit{(Extreme Machine Learning)} baseiam-se no Teorema de Cover para realizar um remapeamento das features de entrada em um plano linearmente separável, onde a matriz de pesos de remapeamento é obtida de forma aleatória. Desta forma, ao remapear o  conjunto de dados em um um plano de maior dimensonalidade, existe uma alta probabilidade de que as classes sejam, agora, linearmente separáveis. Redes RBF, por sua vez, fazem uso de uma função de ativação radial, tipicamente oriundas de processos de clusterização, para a inicialização dos pesos da matriz de remapeamento.


\section{ELM}

<<echo=FALSE>>=
rm(list=ls())
library(pracma)
library(corpcor)
library(mlbench)
library(pROC)
set.seed(42)
@

\subsection{Modelagem de função de OU Exclusivo}

Usaremos ELM para modelar uma função XOR, seguindo a tabela verdade,  para u conjunto de entrada de variáveis:

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[H]
\begin{tabular}{ccc}
\textbf{x1} & \textbf{x2} & \textbf{y} \\ \hline
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0
\end{tabular}
\end{table}

<<echo=TRUE, fig=F>>=
load("data2classXOR.txt")

rows <- dim(X)[1]
features <- dim(X)[2]

@

As features das observações podem ser entendidas como sendo valores acima ou abaixo de um determinado threshold, podendo ser modeladas como valores reais, ao invés de entradas binárias. Digamos que qualquer valor acima de 3 seja classificado como sendo 1, e abaixo como sendo 0. Desta forma a divisão das amostras pode ser vista conforme abaixo, onde a classe azul corresponde às variáveis cuja saída é 1, e a vermelha, cuja saída é 0 (ou como obtidas nos experimentos, -1).

<<echo=F, fig=T, fig.cap='Classes'>>=

lim = c(min(X), max(X))

index_1 <- which(Y > 0)
class1 <- X[index_1,]
class2 <-X[-index_1,]
plot(class1[,1], class1[,2], xlim=lim, ylim=lim, col='blue', xlab='x1', ylab='x2')
par(new=T)
plot(class2[,1], class2[,2], xlim=lim, ylim=lim, col='red', xlab='', ylab='')
@


A rede implementada possui uma camada escondida, com 5 neurônios. O que siginifa que mapearemos cada variável, de um espaço de duas variáveis para um espaço de 5. Os pesos da matriz Z responsável pelo mapeamento das variáveis para os neurônios da camada intermediária é inicializado de forma aleatória. A partir disso, uma nova matriz de saída desta camada, remapeada, é gerada. A matriz possui dimensões iguais à quantidade de amostras de entrada pela quantidade de features novas correspondentes (número de neurônios).

<<echo=T>>=
# Number of neurons in hidden layer
p <- 5
Z <- replicate(p, runif(features+1, -0.5, 0.5))

print('Matriz Z')
print(Z)

# Adição de um termo correspondente ao bias na entrada de cada neurônio
Xaug <- cbind(replicate(rows, 1), X)

H <- tanh(Xaug %*% Z)
@

O cálculo da matriz de pesos, responsável por remapear as amostras, deste novo hiperplano, para os rótulos de classes corretos, é realizado através de mínimos quadrados, da saída com a pseudoinversa da saída da camada intermediária:

<<echo=T>>=
W <- pseudoinverse(H) %*% Y
@

O erro para o modelo calculado é dados por:

<<echo=T>>=
# Calculate Error
Y_hat <- sign(H %*% W)
err <- sum((Y-Y_hat)^2)/4
print(err)
@

Aplicando o modelo para um conjunto de teste, o erro calculado é de:

<<echo=T>>=
# Test
Xaug <- cbind(replicate(features * 4, 1), X_t)
Ht <- tanh(Xaug %*% Z)
Y_hat_t <- sign(Ht %*% W)
err_t <- sum((Y_t-Y_hat_t)^2)/4
print(err_t)
@

A aplicação de ELM determina que mapeamos as amostras para um  espaço de dimmensionalidade muito maior do que o espaço original. Para 10 neurônios na camada escondida, temos:

<<echo=F>>=
p <- 10
Z <- replicate(p, runif(features+1, -0.5, 0.5))
Xaug <- cbind(replicate(rows, 1), X)
H <- tanh(Xaug %*% Z)
W <- pseudoinverse(H) %*% Y
Y_hat <- sign(H %*% W)
err <- sum((Y-Y_hat)^2)/4
print(paste('Erro treinamento:', err))

# Test
Xaug <- cbind(replicate(features * 4, 1), X_t)
Ht <- tanh(Xaug %*% Z)
Y_hat_t <- sign(Ht %*% W)
err_t <- sum((Y_t-Y_hat_t)^2)/4
print(paste('Erro teste:',err_t))
@

\noindent
para 50 neurônios,

<<echo=F>>=
p <- 50
Z <- replicate(p, runif(features+1, -0.5, 0.5))
Xaug <- cbind(replicate(rows, 1), X)
H <- tanh(Xaug %*% Z)
W <- pseudoinverse(H) %*% Y
Y_hat <- sign(H %*% W)
err <- sum((Y-Y_hat)^2)/4
print(paste('Erro:', err))

# Test
Xaug <- cbind(replicate(features * 4, 1), X_t)
Ht <- tanh(Xaug %*% Z)
Y_hat_t <- sign(Ht %*% W)
err_t <- sum((Y_t-Y_hat_t)^2)/4
print(paste('Erro teste:',err_t))
@


\noindent
para 100 neurônios,

<<echo=F>>=
p <- 100
Z <- replicate(p, runif(features+1, -0.5, 0.5))
Xaug <- cbind(replicate(rows, 1), X)
H <- tanh(Xaug %*% Z)
W <- pseudoinverse(H) %*% Y
Y_hat <- sign(H %*% W)
err <- sum((Y-Y_hat)^2)/4
print(paste('Erro:', err))

# Test
Xaug <- cbind(replicate(features * 4, 1), X_t)
Ht <- tanh(Xaug %*% Z)
Y_hat_t <- sign(Ht %*% W)
err_t <- sum((Y_t-Y_hat_t)^2)/4
print(paste('Erro teste:',err_t))
@

\noindent
para 500 neurônios,

<<echo=F>>=
p <- 500
Z <- replicate(p, runif(features+1, -0.5, 0.5))
Xaug <- cbind(replicate(rows, 1), X)
H <- tanh(Xaug %*% Z)
W <- pseudoinverse(H) %*% Y
Y_hat <- sign(H %*% W)
err <- sum((Y-Y_hat)^2)/4
print(paste('Erro:', err))

# Test
Xaug <- cbind(replicate(features * 4, 1), X_t)
Ht <- tanh(Xaug %*% Z)
Y_hat_t <- sign(Ht %*% W)
err_t <- sum((Y_t-Y_hat_t)^2)/4
print(paste('Erro teste:',err_t))
@

De forma geral, temos, para o treinamento, a seguinte :


<<echo=F, fig=T>>=
errs_train <- c()
errs_test <- c()
for (p in seq(1, 1000, 10)) {
    Z <- replicate(p, runif(features+1, -0.5, 0.5))
    Xaug <- cbind(replicate(rows, 1), X)
    H <- tanh(Xaug %*% Z)
    W <- pseudoinverse(H) %*% Y
    Y_hat <- sign(H %*% W)
    err <- sum((Y-Y_hat)^2)/4
    errs_train <- c(errs_train, err)

    # Test
    Xaug <- cbind(replicate(features * 4, 1), X_t)
    Ht <- tanh(Xaug %*% Z)
    Y_hat_t <- sign(Ht %*% W)
    err_t <- sum((Y_t-Y_hat_t)^2)/4
    errs_test <- c(errs_test, err_t)
}

plot(seq(1, 1000, 10), errs_train, xlab='Números de neurônios na camada intermediária para treinamento', ylab='Erros', type='l')

@

E, para as amostras de teste,

<<echo=F, fig=T>>=
plot(seq(1, 1000, 10), errs_test, xlab='Números de neurônios na camada intermediária para teste', ylab='Erros', type='l')
@

Os gráficos dos erros de treinamento e teste indicam que, quanto maior a quantidade de neurônios na camada intermediária, ou seja, maior a dimensionalidade do novo espaço, menor o erro de trinamento, ou seja, maior a separabilidade das classes, porém uma maior oscilação nos erros dos testes, o que pode ser um indicativo de overfitting no modelo.

\subsection{Wisconsin Breast Cancer}

O modelo de redes neurais multicamadas ELM é, agora, utilizado para treinamento de dataset Wisconsin Breast Cancer. O dataset foi dividido em treinamento e teste.

<<echo=T>>=
rm(list=ls())
data("BreastCancer")

bc <- BreastCancer[complete.cases(BreastCancer),]
x <- bc[, 2:10]
y <- bc[,11]

x <- sapply(x, as.numeric)
y <- replicate(dim(bc)[1], 0)
y[which(bc[,11]== 'benign')] = -1
y[which(bc[,11]== 'malignant')] = 1

index_train = sample(seq(dim(x)[1]), as.integer(0.7*dim(x)[1]), replace=FALSE)
x_train = x[index_train,]
x_test = x[-index_train,]

y_train <- y[index_train]
y_test = y[-index_train]
@

Para avaliar o resultado das operações, será feito uso de uma função de performance, especificada como abaixo:

<<echo=T>>=
evaluate <- function(y, y_hat) {

    y[which(y<0)] = 0
    y_hat[which(y_hat<0)] = 0

    errors <- y - y_hat
    false_positive <- length(errors[errors < 0])
    true_positive <- length(y[y[which(errors == 0)] > 0])
    false_negative <- length(errors[errors > 0])
    true_negative <- length(y[y[which(errors == 0)] <= 0])

    confusion_matrix <- matrix(replicate(4, 0), nrow = 2, ncol = 2)
    confusion_matrix[1,1] <- true_positive
    confusion_matrix[1,2] <- false_positive
    confusion_matrix[2,1] <- false_negative
    confusion_matrix[2,2] <- true_negative

    return(list(
        'errors' = errors,
        'error'= mean(abs(errors)),
        'accuracy' = (1 - mean(abs(errors))),
        'specitivity'= true_negative / (true_negative + false_positive),
        'sensibility' = true_positive / (true_positive + false_negative),
        'confusion_matrix' = confusion_matrix
    ))
}
@


Realizando o treinamento, temos as seguintes métricas como resultado.

<<echo=F, fig=F>>=
rows <- dim(x_train)[1]
features <- dim(x_train)[2]

# Number of neurons in hidden layer
p <- 20
Z <- replicate(p, runif(features+1, -0.5, 0.5))
Xaug <- cbind(replicate(rows, 1), x_train)

H <- tanh(Xaug %*% Z)
W <- pseudoinverse(H) %*% y_train

# Calculate Error
Y_hat <- sign(H %*% W)

result <- evaluate(y_train, Y_hat)

sprintf('Porcentagem de erro: %f', result$error)
sprintf('Acurácia: %f', result$accuracy)
sprintf('Sensibiliade: %f', result$sensibility)
sprintf('Especificidade: %f', result$specitivity)
@

As métricas de teste obtidas mostram um bom desempenho do modelo para a predição de câncer de mama, com as features especificadas. A matriz de confusão obtida pode ser vista a seguir:

<<echo=F>>=
print(result$confusion_matrix)
@

A matriz de confusão corresponde a:

\begin{table}[H]
\begin{tabular}{|l|l|}
\hline
True Positive & False Positive \\ \hline
False Negative & True Negative \\ \hline
\end{tabular}
\end{table}

Para os dados de teste, temos:

<<echo=F, fig=F>>=
rows <- dim(x_test)[1]
features <- dim(x_test)[2]
Xaug <- cbind(replicate(rows, 1), x_test)
H <- tanh(Xaug %*% Z)
Y_hat_t <- sign(H %*% W)

result <- evaluate(y_test, Y_hat_t)

sprintf('Porcentagem de erro: %f', result$error)
sprintf('Acurácia: %f', result$accuracy)
sprintf('Sensibiliade: %f', result$sensibility)
sprintf('Especificidade: %f', result$specitivity)
@

E a matriz de confusão:

<<echo=F>>=
print(result$confusion_matrix)
@

\section{RBF}

<<echo=F>>=
rm(list=ls())
library(pracma)
library(corpcor)
library(mlbench)
library(stats)
load("data2classXOR.txt")
@


Redes RBF, como mencionado anteriormente, utilizam funções de ativação radial para os neurônios da camada escondida. A saída da rede, constituindo-se de um valor único, é definida como sendo:

\begin{equation}
\varphi(\overline{X}) = \sum^{N}_{i=1} \alpha_i \rho(|| \overline(x) - \overline(c)_i ||)
\end{equation}

\noindent
onde $\varphi$ corresponde à função de ativação do neurônio de saída, neste caso uma função de identidade de tangente hiperbólica, N corresponde ao número de neurônios na camada escondida, $\alpha_i$ corresponde ao peso deste neurônio na soma ponderada para a camada de saída, $c_i$ corresponde à posição de cada centróide e $\rho$ é uma função radial gaussiana, definida, por sua vez, como:

\begin{equation}
\rho(|| \overline(x) - \overline(c)_i ||) = \exp(-\beta(|| \overline(x) - \overline(c)_i ||)^2)
\end{equation}

Em outras palavras, cada neurônio da camada escondida pode corresponder a um centróide de um processo de clusterização das amostras. A saída de cada um desses neurônios, para uma observação, corresponde à distância euclidiana da observação ao centróide do neurônio em questão, após passar por uma função de ativação gaussiana, com um parâmetro de regularização $\beta$. Com isso, cada amostra é mapeada de um espaço com as dimensões originais das amostras, para um espaço de dimensões iguais à quantidade de neurônios utilizados na camada escondida. Definimos uma função radial da forma:

<<echo=T>>=
radial_activation <- function (X, centers, beta=0.1) {
    n_samples <- dim(X)[1]
    features <- dim(X)[2]

    n_centers <- dim(centers)[2]

    result <- matrix(nrow=n_samples, ncol=n_centers)

    for (sample in seq(n_samples)) {
        for (center in seq(n_centers)) {
            result[sample, center] <- exp(
                -beta * dist(rbind(X[sample,], centers[,center]))**2
                )
        }
    }

    return(result)
}
@

O algoritmo utilizado para o treinamento dos dados, portanto, é:

<<echo=T>>=

rows <- dim(X)[1]
features <- dim(X)[2]

# Number of neurons in hidden layer
p <- 5

# Training
Xaug <- cbind(replicate(rows, 1), X)

centroids <- kmeans(Xaug, p)
centers <- t(centroids$centers)

H <- radial_activation(Xaug, centers)

# H <- tanh(Xaug %*% Z)
W <- pseudoinverse(H) %*% Y
@

O desempenho do treinamento realizado pode ser observado ao aplicarmos o cálculo com os pesos no conjunto de treinamento, e avaliar o erro associado:

<<echo=T>>=
# Calculate Error
Y_hat <- sign(H %*% W)
err <- sum((Y-Y_hat)^2)/4
print(err)
@

Aplicando o modelo para um conjunto de teste, o erro calculado é de:

<<echo=T>>=
# Test
Xaug <- cbind(replicate(features * 4, 1), X_t)
Ht <- radial_activation(Xaug, centers)
Y_hat_t <- sign(Ht %*% W)
err_t <- sum((Y_t-Y_hat_t)^2)/4
print(err_t)
@

Podemos realizar uma análise similar à realizada para ELM, considerando a influência do número de neurônios na camada escondida com o desempenho da rede implementada. Portando, para um conjunto de treinamento, temos o seguinte gráfico de desempenho:

<<echo=F, fig=T>>=
errs_train <- c()
errs_test <- c()
n_samples <- dim(X)[1]
for (p in seq(1, n_samples-1)) {
    Xaug <- cbind(replicate(rows, 1), X)
    centroids <- kmeans(Xaug, p)
    centers <- t(centroids$centers)
    H <- radial_activation(Xaug, centers)
    W <- pseudoinverse(H) %*% Y
    Y_hat <- sign(H %*% W)
    err <- sum((Y-Y_hat)^2)/4
    errs_train <- c(errs_train, err)

    # Test
    Xaug <- cbind(replicate(features * 4, 1), X_t)
    Ht <- radial_activation(Xaug, centers)
    Y_hat_t <- sign(Ht %*% W)
    err_t <- sum((Y_t-Y_hat_t)^2)/4
    errs_test <- c(errs_test, err_t)
}

plot(seq(1, n_samples-1), errs_train, xlab='Números de neurônios na camada intermediária para treinamento', ylab='Erros', type='l')

@

E, para as amostras de teste,

<<echo=F, fig=T>>=
plot(seq(1, n_samples-1), errs_test, xlab='Números de neurônios na camada intermediária para teste', ylab='Erros', type='l')
@

O número de neurônios máximo que podemos definir é limitado pelo número máximo de amostras disponíveis, uma vez que  seria impossível definir mais centros do que amostras. Pelos gráficos, podemos observar que um aumento do número de centróides causa uma diminuição do desempenho da rede, devido a um overfitting dos dados, umva vez que este decaimento de desempenho não é obervado no treinamento.

\subsection{Wisconsin Breast Cancer}

O modelo de redes neurais multicamadas RBF é, agora, utilizado para treinamento de dataset Wisconsin Breast Cancer. O dataset foi dividido em treinamento e teste.

<<echo=T>>=
rm(list = setdiff(ls(), lsf.str()))
data("BreastCancer")

bc <- BreastCancer[complete.cases(BreastCancer),]
x <- bc[, 2:10]
y <- bc[,11]

x <- sapply(x, as.numeric)
y <- replicate(dim(bc)[1], 0)
y[which(bc[,11]== 'benign')] = -1
y[which(bc[,11]== 'malignant')] = 1

index_train = sample(seq(dim(x)[1]), as.integer(0.7*dim(x)[1]), replace=FALSE)
x_train = x[index_train,]
x_test = x[-index_train,]

y_train <- y[index_train]
y_test = y[-index_train]
@

Para avaliar o resultado das operações, será feito uso novamente de uma função de performance como especificada anteriormente:

<<echo=T>>=
evaluate <- function(y, y_hat) {

    y[which(y<0)] = 0
    y_hat[which(y_hat<0)] = 0

    errors <- y - y_hat
    false_positive <- length(errors[errors < 0])
    true_positive <- length(y[y[which(errors == 0)] > 0])
    false_negative <- length(errors[errors > 0])
    true_negative <- length(y[y[which(errors == 0)] <= 0])

    confusion_matrix <- matrix(replicate(4, 0), nrow = 2, ncol = 2)
    confusion_matrix[1,1] <- true_positive
    confusion_matrix[1,2] <- false_positive
    confusion_matrix[2,1] <- false_negative
    confusion_matrix[2,2] <- true_negative

    return(list(
        'errors' = errors,
        'error'= mean(abs(errors)),
        'accuracy' = (1 - mean(abs(errors))),
        'specitivity'= true_negative / (true_negative + false_positive),
        'sensibility' = true_positive / (true_positive + false_negative),
        'confusion_matrix' = confusion_matrix
    ))
}
@


Realizando o treinamento, temos as seguintes métricas como resultado.

<<echo=F, fig=F>>=
rows <- dim(x_train)[1]
features <- dim(x_train)[2]

# Number of neurons in hidden layer
p <- 5

# Training
Xaug <- cbind(replicate(rows, 1), x_train)
centroids <- kmeans(Xaug, p)
centers <- t(centroids$centers)
H <- radial_activation(Xaug, centers)
W <- pseudoinverse(H) %*% y_train

# Calculate Error
Y_hat <- sign(H %*% W)

result <- evaluate(y_train, Y_hat)

sprintf('Porcentagem de erro: %f', result$error)
sprintf('Acurácia: %f', result$accuracy)
sprintf('Sensibiliade: %f', result$sensibility)
sprintf('Especificidade: %f', result$specitivity)
@

As métricas de teste obtidas mostram um bom desempenho também do modelo em RBF para a predição de câncer de mama, com as features especificadas. A matriz de confusão obtida pode ser vista a seguir:

<<echo=F>>=
print(result$confusion_matrix)
@

A matriz de confusão corresponde a:

\begin{table}[H]
\begin{tabular}{|l|l|}
\hline
True Positive & False Positive \\ \hline
False Negative & True Negative \\ \hline
\end{tabular}
\end{table}

Para os dados de teste, temos:

<<echo=F, fig=F>>=
rows <- dim(x_test)[1]
features <- dim(x_test)[2]
Xaug <- cbind(replicate(rows, 1), x_test)
centers <- t(centroids$centers)
Ht <- radial_activation(Xaug, centers)
Y_hat_t <- sign(Ht %*% W)

result <- evaluate(y_test, Y_hat_t)

sprintf('Porcentagem de erro: %f', result$error)
sprintf('Acurácia: %f', result$accuracy)
sprintf('Sensibiliade: %f', result$sensibility)
sprintf('Especificidade: %f', result$specitivity)
@

E a matriz de confusão:

<<echo=F>>=
print(result$confusion_matrix)
@

\begin{thebibliography}{1}

	%Each item starts with a \bibitem{reference} command and the details thereafter.
	\bibitem{specificity} % Transaction paper
	ML Metrics: Sensitivity vs. Specificity -
	\url{https://dzone.com/articles/ml-metrics-sensitivity-vs-specificity-difference}.
	Acessado em 28 de agosto de 2019.


\end{thebibliography}

\end{document}
